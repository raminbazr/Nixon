{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the given example, grammar, which is defined using a simple regular expression rule. \n",
    "#This rule says that an NP (Noun Phrase) chunk should be formed whenever the chunker finds an optionn\n",
    "#al determiner (DT) followed by any number of adjectives (JJ) and then a noun (NN).\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk import pos_tag \n",
    "file = open('c:/Users/Ramin/Desktop/Nixon.txt','r+')\n",
    "text =file.read()  \n",
    "# define chunking function with text and regular \n",
    "# expression representing grammar as parameter \n",
    "def chunking(text, grammar): \n",
    "    word_tokens = word_tokenize(text) \n",
    "# label words with part of speech \n",
    "    word_pos = pos_tag(word_tokens) \n",
    "    # create a chunk parser using grammar \n",
    "    chunkParser = nltk.RegexpParser(grammar) \n",
    "    # test it on the list of word tokens with tagged pos \n",
    "    tree = chunkParser.parse(word_pos) \n",
    "      \n",
    "    for subtree in tree.subtrees(): \n",
    "        print(subtree) \n",
    "    tree.draw() \n",
    "      \n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "chunking(text, grammar) \n",
    "################################################################\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk import pos_tag \n",
    "# extract information about the tag \n",
    "#nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset('NN') #میگه مثلا ان-ان یعنی چی؟\n",
    "nltk.help.upenn_tagset('NNP') \n",
    "nltk.help.upenn_tagset('To') \n",
    "# convert text into word_tokens with their tags \n",
    "def pos_tagging(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    return pos_tag(word_tokens) \n",
    "pos_tagging(text) \n",
    "################################################################\n",
    "# remove whitespace from text \n",
    "def remove_whitespace(text): \n",
    "    return  \" \".join(text.split())   \n",
    "remove_whitespace(text)\n",
    "#################################################################\n",
    "# remove stopwords function \n",
    "def remove_stopwords(text): \n",
    "    stop_words = set(stopwords.words(\"english\")) \n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words] \n",
    "    return filtered_text \n",
    "remove_stopwords(text)\n",
    "################################################################\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "stemmer = PorterStemmer() \n",
    "# stem words in the list of tokenised words \n",
    "def stem_words(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    stems = [stemmer.stem(word) for word in word_tokens] \n",
    "    return stems \n",
    "stem_words(text) \n",
    "#################################################################\n",
    "import nltk\n",
    "import re\n",
    "import pprint\n",
    "tokens= nltk.word_tokenize(text)\n",
    "print(tokens)\n",
    "sentence = nltk.sent_tokenize(text)\n",
    "print(sentence)\n",
    "print(text)\n",
    "stemmer = nltk.stem.WordNetLemmatizer()\n",
    "\" \".join(stemmer.lemmatize(text))\n",
    "########################################################################\n",
    "#Removing Accented Characters, for example: ('Sómě Áccěntěd těxt') = 'Some Accented text'\n",
    "file = open ('C:/Users/Ramin/Desktop/Nixon.txt','r+')\n",
    "text = file.read()\n",
    "import unicodedata\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode(\"ascii\",\"ignore\").decode('utf-8', 'ignore')\n",
    "    return text\n",
    "remove_accented_chars(text)\n",
    "##########################################################################\n",
    "import nltk\n",
    "import numpy as np\n",
    "file = open ('C:/Users/Ramin/Desktop/Nixon.txt','r+')\n",
    "text = file.read()\n",
    "def tokenize_text(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "sents = tokenize_text(text)\n",
    "np.array(sents)\n",
    "##############################################################################\n",
    "import nltk\n",
    "import numpy as np\n",
    "file = open('c:/Users/Ramin/Desktop/Nixon.txt','r+')\n",
    "text =file.read()\n",
    "# pattern to identify tokens by using gaps between tokens\n",
    "GAP_PATTERN = r'\\s+'\n",
    "regex_wt = nltk.RegexpTokenizer(pattern=GAP_PATTERN,\n",
    "gaps=True)\n",
    "words = regex_wt.tokenize(text)\n",
    "np.array(words)\n",
    "#to obtain the token boundaries for each token during the tokenize operation.\n",
    "word_indices = list(regex_wt.span_tokenize(text))\n",
    "print(word_indices)\n",
    "print(np.array([text[start:end] for start, end in word_indices]))\n",
    "################################################################################\n",
    "import nltk\n",
    "import numpy as np\n",
    "file = open('c:/Users/Ramin/Desktop/Nixon.txt','r+')\n",
    "text =file.read()\n",
    "TOKEN_PATTERN = r'\\w+'\n",
    "regex_wt = nltk.RegexpTokenizer(pattern=TOKEN_PATTERN,\n",
    "gaps=False)\n",
    "words = regex_wt.tokenize(text)\n",
    "np.array(words)\n",
    "#################################################################################\n",
    "file = open('c:/Users/Ramin/Desktop/Nixon.txt','r+')\n",
    "text =file.read()\n",
    "# lowercase\n",
    "#text.lower()\n",
    "# uppercase\n",
    "#text.upper()\n",
    "# title case\n",
    "text.title()\n",
    "###################################################################################\n",
    "#'My schooool is realllllyyy amaaazingggg' = 'My school is really amazing'\n",
    "#We can see from this output that our function performs as intended and replaces the\n",
    "#repeating characters in each token, giving us correct tokens as desired.\n",
    "from nltk.corpus import wordnet\n",
    "def remove_repeated_characters(tokens):\n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    match_substitution = r'\\1\\2\\3'\n",
    "    def replace(old_word):\n",
    "        if wordnet.synsets(old_word):\n",
    "            return old_word\n",
    "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "        return replace(new_word) if new_word != old_word else new_word\n",
    "    correct_tokens = [replace(word) for word in tokens]\n",
    "    return correct_tokens\n",
    "correct_tokens = remove_repeated_characters(nltk.word_tokenize(text))\n",
    "' '.join(correct_tokens)\n",
    "###################################################################################\n",
    "file = open('c:/Users/Ramin/Desktop/Nixon.txt','r+')\n",
    "text =file.read()\n",
    "# POS tagging with nltk\n",
    "import nltk\n",
    "nltk_pos_tagged = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "pd.DataFrame(nltk_pos_tagged, columns=['Word', 'POS tag']).T\n",
    "# POS tagging with Spacy\n",
    "#spacy_pos_tagged = [(word, word.tag_, word.pos_) for word in sentence_nlp]\n",
    "#pd.DataFrame(spacy_pos_tagged, columns=['Word', 'POS tag', 'Tag type']).T\n",
    "######################################################################################\n",
    "#Special characters and symbols are usually non-alphanumeric characters or even\n",
    "#occasionally numeric characters (depending on the problem), which add to the extra\n",
    "#noise in unstructured text. Usually, simple regular expressions (regexes) can be used to\n",
    "#remove them.I’ve kept removing digits optional, because often we might need to keep them in the\n",
    "#preprocessed text.در واقع اعداد و علائم نگارشی و @ اینا را حذف میکنه\n",
    "import re\n",
    "file = open('c:/Users/Ramin/Desktop/Nixon.txt','r+')\n",
    "text =file.read()\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "remove_special_characters(text,\n",
    "remove_digits=True)\n",
    "########################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
